{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAA fall2 blue5 text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "#from os import path\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "#from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up working file\n",
    "file = 'nameofifle.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load excel file\n",
    "tips = pd.ExcelFile(file)\n",
    "#print(type(tips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get excel names excluding the summary sheet\n",
    "tipsyear=tips.sheet_names[1:]\n",
    "\n",
    "# wrap up together\n",
    "tipsdf=pd.DataFrame()\n",
    "\n",
    "for year in tipsyear:\n",
    "    tipdf=tips.parse(year, names=['TIPS'])\n",
    "    tipdf['year'] = year\n",
    "    tipsdf=tipsdf.append(tipdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split TIPS sentences into word list\n",
    "#tipsdf['TIPSsplit'] = tipsdf['TIPS'].str.replace(r'([?!,.]+)', '').str.lower().str.split()\n",
    "\n",
    "# clean by remove punctuation\n",
    "tipsdf['TIPS_clean'] = tipsdf['TIPS'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "tipsdf['TIPS_tokenized'] = tipsdf['TIPS_clean'].apply(lambda x: tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(stopwords.words('english'))\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "tipsdf['TIPS_nostop'] = tipsdf['TIPS_tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "tipsdf['TIPS_nostop'] = tipsdf['TIPS'].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tipsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n ):\n",
    "    n_grams = ngrams(word_tokenize(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in ngrams(word_tokenize(sentence), n):\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ngrams(example_sent,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(filtered_sentence)\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(a for a in ngrams(word_tokenize(example_sent), 2))\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 2\n",
    "#sixgrams = ngrams(sentence.split(), n)\n",
    "#for grams in sixgrams:\n",
    "#    print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist= {}\n",
    "for tips in tipsdf['TIPSsplit']:\n",
    "    for word in tips:\n",
    "        wordlist[word] = wordlist.get(word,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_words=['when','i',\"you're\",'the','them','they','what','during','a','an','all','is','so','are', 'get','this','be', 'for','you','but','your','and','or','to','of','will','in','as', 'on', 'it','-','go','going','day','off','before','doing','often','most','any','these','then',\"won't\",'was','else','how','me','may','year','would','someone','we','two','had','because','has','might','no','could','here','also','iaa','put','even','well','back','away','too','having',\"don't\",'throuhg','while', \n",
    "#              'by','with','that','at','about','have','if','out','from','can','not', 'do', \"don't\", 'The','work','much',\"it's\",'up','really','than','more','did','class','always','just','their','way','some','lot','very','one',\"you'll\",'something','everything','only','into','first','after','each','getting','other','others','good','keep','many','there','every','want','things'] + ['job','program','help','make','take','3','yourself']\n",
    "\n",
    "wordlist_cleaned = {key: wordlist[key] for key in wordlist if key not in common_words}\n",
    "\n",
    "#wordcloud=WordCloud(width=800, height=600).generate_from_frequencies(wordlist_cleaned)\n",
    "#plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "_=WordCloud(width=1920, height=1080, max_font_size=256,background_color='white').generate_from_frequencies(wordlist_cleaned).to_file('/Users/Quantum/Downloads/T.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist['-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud=WordCloud(stopwords=None).generate_from_frequencies(wordlist)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#plt.axis(\"off\")\n",
    "\n",
    "# lower max_font_size\n",
    "#wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "#plt.figure()\n",
    "#plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "# The pil way (if you don't have matplotlib)\n",
    "# image = wordcloud.to_image()\n",
    "# image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
